{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stylometric Analysis for Akkadian Cuneiform Texts\n",
    "\n",
    "This Jupyter notebook provides code to perform stylometric analysis on cuneiform texts. The code was designed specifically for Akkadian texts, however, it is likely to work well on all languages that used the cuneiform script, meaning all languages that have a transliteration system that follows the same principles. Additionally, texts written in other non-alphabetic writing systems for whose characters there are unicode representations could probably benefit from this code as well. The code is based on [*Introduction to stylometry with python* from the Programming Historian website](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python).\n",
    "\n",
    "It is accompanied by a metadata folder and texts folders. The texts folder currently holds one corpus as a case-study, 81 legal and administrative documents from the Neo-Babylonian period (ca. 7^th^-5^th^ centuries BCE). These texts edited in the 1975 dissertation of Raymond B. Dillard. They are currently in the Free Library of Philadelphia (FLP), after being purchased on the antiquities market in the early 20^th^ century by John Frederik Lewis.\n",
    "\n",
    "The notebook employs the following methods on the texts:\n",
    "\n",
    "1. Tf-idf vectorization\n",
    "2. Create a distance matrix between the texts\n",
    "3. T-SNE dimenstion reduction to display the distances between the texts on a scatter plot\n",
    "4. Network analysis of the relationships between similar texts\n",
    "5. A close reading: looking at groupings of nearest texts\n",
    "\n",
    "The code is written so that no previous knowledge in python or stylometric methods is assumed. There are instructions throughout that explain the code, the stylometric methods, and give the necessary information to choose specific parameters.\n",
    "\n",
    "As different parameters can give different results, it is possible to run the code several times and assess the differences.\n",
    "\n",
    "Certain parameters are decided in advance (e.g. not to lowercase all characters) and some are left to the user (e.g. choosing 1-gram, 2-gram, or 3-gram). The idea behind this was that certain parameters are necessary for a logical parsing of cuneiform transliterations or their representation in Unicode cuneiform glyphs.\n",
    "\n",
    "## Adding corpora\n",
    "\n",
    "The stylometric analysis can be performed on texts which are either transliterated, or in which the signs are represented in Unicode. For transliterated texts, it is recommended that personal names and numbers be replaced with PN and NUM (or similar), depending on the focus of study (i.e. period, genre, scribal habits etc.).\n",
    "\n",
    "The new corpora need to be added to the relevant folders before starting to use the code. This includes the following:\n",
    "\n",
    "1. A folder which includes the texts for stylometric analysis:\n",
    "- The folder should be placed under the `texts` folder.\n",
    "- Each file should be a plain text file and contain one text only.\n",
    "- The texts should be transliterated texts without any editorial marks (including no hyphens and dots between signs), or texts in Unicode cuneiform.\n",
    "- It is recommended that each file name will be the name of the text (this is later used as the text identifier in the dataframes and CSVs produced).\n",
    "2. A metadata file\n",
    "- The file should be placed under the `metadata` folder.\n",
    "- The file should be a CSV file.\n",
    "- One of the columns **needs to be titled** `text_name`, and its values need to be identical to the file names of the texts under the `texts` folder.\n",
    "- It is recommended that additional columns will have relevant information about the texts you want to study (period, genre, area, scribe, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Importing Libraries and Defining Functions\n",
    "\n",
    "The following five blocks of code import the python libraries needed and define functions which will be called later in the code.\n",
    "\n",
    "A short explanation precedes each function to explain what its different parameters mean. A fuller explanation of what the function do appears below when they are used in the main code.\n",
    "\n",
    "You will only need to run these initial five code blocks once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import *\n",
    "import panel as pn\n",
    "import panel.widgets as pnw\n",
    "pn.extension()\n",
    "pn.extension(\"plotly\", \"tabulator\")\n",
    "import holoviews as hv\n",
    "from holoviews import dim, opts\n",
    "hv.extension(\"bokeh\", \"matplotlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### vectorize() function\n",
    "\n",
    "This function uses the tf-idf Vectorizer from sklearn library. \n",
    "\n",
    "Two parameters are set: the _input_ is `filename` and _lowercase_ is set to `False`.\n",
    "\n",
    "The function takes the following variables:\n",
    "\n",
    "`file-paths`: the full paths to the text files\n",
    "\n",
    "`analyzer`: word, char, or char-wb, see futher below\n",
    "\n",
    "`ngram-range`: see further below\n",
    "\n",
    "`max_df`: see further below\n",
    "\n",
    "`min_df`: see further below\n",
    "\n",
    "`max-features`: see further below\n",
    "\n",
    "`file-keys`: the file names which are used as keys in the metadata dataframe\n",
    "\n",
    "The function returns a list with the following:\n",
    "\n",
    "`counts`: the tf-idf score of every word for every text as an array\n",
    "\n",
    "`counts_df`: the tf-idf score of every word for every text as a dataframe, where the columns are the file names and the columns are the words in the vocabulary\n",
    "\n",
    "`vocab_df`: a dataframe of the vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(file_paths, analyzer, ngram_range, max_df, min_df, max_features, file_keys):\n",
    "    # returning the parameters that decide the size of the features to their factory settings if none were chosen.\n",
    "    if max_df == False:\n",
    "        max_df = 1.0\n",
    "    if min_df == False:\n",
    "        min_df = 1\n",
    "    if max_features == False:\n",
    "        max_features = None\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(input=\"filename\", lowercase=False, analyzer=analyzer, ngram_range=ngram_range, max_df=max_df, min_df=min_df, max_features=max_features)\n",
    "    counts = vectorizer.fit_transform(file_paths).toarray()\n",
    "    # saving the vocab used for vectorization, and switching the dictionary so that the feature index is the key\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    switched_vocab = {value:key for key, value in vocab.items()}\n",
    "    # adding the vocab words to the counts dataframe for easier viewing.\n",
    "    column_names = []\n",
    "    x=0\n",
    "    while x < len(switched_vocab):\n",
    "        column_names.append(switched_vocab[x])\n",
    "        x+=1\n",
    "    \n",
    "    vocab_df = pd.DataFrame(vocab, index=[\"feature index\"]).transpose()\n",
    "    counts_df = pd.DataFrame(counts, index=file_keys, columns=column_names)\n",
    "    \n",
    "    \n",
    "    return [counts, counts_df, vocab_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distance_calculator() function\n",
    "\n",
    "Calculates the distances between texts based on their tf-idf counts, using the scipy library.\n",
    "\n",
    "Takes the following variables:\n",
    "\n",
    "`counts`: tf-idf counts, calculated using the previous function\n",
    "\n",
    "`metric`: the metric by which to calculate the distances. A list of the different available metrices is given below.\n",
    "\n",
    "`file-keys`: the file names that are used to identify the texts in the dataframe produced.\n",
    "\n",
    "The function returns a dataframe which contains a matrix of the distances between each text to every other text in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_calculator(counts, metric, file_keys):\n",
    "    return pd.DataFrame(squareform(pdist(counts, metric=metric)), index=file_keys, columns=file_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce_dimensions() function\n",
    "\n",
    "This function reduces vectors in multiple dimensions to two dimensional vectors.\n",
    "\n",
    "It takes the following variables:\n",
    "\n",
    "`df`: the dataframe which holds the matrix of relative distances between the texts in the corpus.\n",
    "\n",
    "`file_keys`: the names of the texts used as an index.\n",
    "\n",
    "`perplexity`: see further below.\n",
    "\n",
    "`n_iter`: see further below.\n",
    "\n",
    "`metric`: see further below.\n",
    "\n",
    "The function returns a list with the following:\n",
    "\n",
    "`reduced_df`: a dataframe which holds the results of the two dimensional vectors.\n",
    "\n",
    "`kl_divergence`: the Kullback–Leibler divergence score.\n",
    "\n",
    "`n_features_in`: the number of features (texts) in the given corpus found by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(df, file_keys, perplexity, n_iter, metric):\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, metric=metric, init=\"pca\")\n",
    "    reduced_data = tsne.fit_transform(df)\n",
    "    kl_divergence = tsne.kl_divergence_\n",
    "    n_features_in = tsne.n_features_in_\n",
    "    reduced_df = pd.DataFrame(data=reduced_data, index=file_keys, columns=[\"component 1\", \"component 2\"])\n",
    "    \n",
    "    return [reduced_df, kl_divergence, n_features_in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tf-idf Vectorization and Distance Matrix\n",
    "\n",
    "Running the following code block will upload a widget in which you can choose the corpus you want to use, and the corresponding metadata file. It also allows you to choose parameters for the vectorization of the corpus and for calculating the distances between all the texts in the corpus. The results of the distances between the texts is viewed through a heatmap on the second tab of the widget. Changing the parameters dynamically updates the visualization results.\n",
    "\n",
    "Before the codeblock, read carefully the explanation below to understand what the different parameters and choices mean.\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "Our first step is to vectorize the texts - turn the words or characters of each text into meaningful combinations of numbers, creating a vector for each text. This is done using [Bag-of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) and [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), through the sklearn library. The tf-idf gives a numerical value that represents the relative importance of any given word or character in the document and in the entire corpus.\n",
    "\n",
    "Most of the basic parameters of sklearn's vectorizer were kept at default. One was changed for the purposes of cuneiform transliteration. The default settings of the vectorizer turn all words to lowercase, but in the case of cuneiform transliterations, where the distinction between logograms (written in caps) and phonetic readings (written in lowercase) are important, this was changed.\n",
    "\n",
    "There are three additional parameters to choose values for. They decide how the texts are tokenized and how many tokens to keep:\n",
    "\n",
    "1. analyzer\n",
    "2. n_gram range\n",
    "3. max features / max_df / min_df\n",
    "\n",
    "#### analyzer\n",
    "\n",
    "The analyzer parameter decides whether to tokenize the texts on the character or the word level. There are three options:\n",
    "\n",
    "1. `word`: tokenization of each word, i.e. everything that is separated by a space.\n",
    "2. `char`: tokenization on the character level, where sequences of adjacent characters (a sequence in a length of your choosing, see n_gram range) are tokenized as \"words\". This setting does not take into consideration word boundary, meaning it will put together sequences of characters even if they are in two different words. This setting is recommended for Unicode cuneiform input.\n",
    "3. `char-wb`: this tokenization is like the previous, except that **it does** take into consideration word boundaries. Characters at the end or beginning of words will be padded with space if needed, depending on the n_grams chosen (see below). This setting is recommended for Unicode cuneiform input.\n",
    "\n",
    "#### n_gram range\n",
    "\n",
    "The tokens don't have to be standalone words.\n",
    "\n",
    "- If you have chosen `word` as your analyzer, the n_grams will represent the sequence of words grouped together as a token.\n",
    "- If you have chosen `char` or `char-wb`, the n_grams will represent the sequence of characters grouped together as a token.\n",
    "\n",
    "The n_gram parameter requires two numbers: the minimum length of the sequence and the maximum length.\n",
    "\n",
    "- If you wish to have tokens in the lengths of 2 and 3 characters, or 2 and 3 words, your first number will be 2 and the second 3. \n",
    "- If you don't want to have a range of possible sequences, you can put the same number twice. For example, to have only sequences of 3 characters, slide the range slider below to 3.\n",
    "- If you want the features be sequences of 1 word or character, slide it to 1.\n",
    "\n",
    "For good results, it is recommended not to use more then a maximum of 3-grams.\n",
    "\n",
    "#### max features / max_df / min_df\n",
    "\n",
    "It is usually ineffective to take into consideration all the vocabulary, or tokens, in a given corpora: not only is that sometimes computationally intensive, but also calculations can be thrown off by too rare or too common words that don't have real significance.\n",
    "\n",
    "There are three ways to handle the size of your corpus in sklearn's Vectorizer.\n",
    "\n",
    "1. `max_df`: this parameter is an effective method for removing stopwords. It can take an integer number or a float; the code below allows only for a float between 0 and 1, that represents the *percentage of the documents in which a certain term appears*. Words that appear in **more then** a chosen percentage of the texts, will not be used as part of the vectorization. For example, if the `max_df` is set to 0.7, that means that terms that appear in 70% or more of the texts in the corpus, will not be kept as features in the vectorization.\n",
    "2. `min_df`: this parameter is the opposite of max_df, and it is an effective method for removing particularly rare words. It can take an integer number or a float; the code below allows only for a float between 0 and 1, that represents the *percentage of the documents in which a certain term appears*. Words that appear in **less then** a chosen percentage of the texts, will not be used as part of the vectorization. For example, if the `min_df` is set to 0.1, terms that appear in 10% or less of the texts in the corpus, will not be used as features for vectorization. However, **if `min_df` is set to 1**, all terms that appear in at least one document are used, meaning, this is the default setting that does not ignore any of the terms.\n",
    "3. `max features`: the maximum number of terms used for vectorization. After taking into consideration `max_df` (if that parameter was given), it chooses the x most important terms in the corpus based on term frequency. Meaning, if 1000 is chosen for `max features` (and `max_df` was left at default), that means that the vectorization will use the 1000 features that have the highest term frequency.\n",
    "\n",
    "### Calculating distances between vectors\n",
    "\n",
    "Once texts are turned into vectors, it is possible to calculate their distance from each other. In this notebook, the `pdist` function from the scipy python library is used. \n",
    "\n",
    "There is one parameter to decide for the `pdist` function, and that is the metric by which we measure the distance between each pair of vectors. \n",
    "\n",
    "The following metrices for measuring distance are available in scipy library:\n",
    "\n",
    "‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘jensenshannon’, ‘kulsinski’, ‘kulczynski1’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n",
    "\n",
    "A description of how the different distances are calculated is available at [scipy's documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html). Usually, it is recommended to use the ‘euclidean’ or ‘cosine’ metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# text_options = os.listdir(\"texts/\")\n",
    "# metadata_options = os.listdir(\"metadata/\")\n",
    "metrices = ['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'jensenshannon', 'kulsinski', 'kulczynski1', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
    "\n",
    "folder_name = pnw.Select(name=\"Corpus\", value=os.listdir(\"texts/\")[0], options=os.listdir(\"texts/\"))\n",
    "metadata_name = pnw.Select(name=\"Corpus\", value=os.listdir(\"metadata/\")[0], options=os.listdir(\"metadata/\"))\n",
    "analyzer = pnw.Select(name=\"Analyzer\", value=\"word\", options=[\"word\", \"char\", \"char_wb\"])\n",
    "n_gram = pnw.IntRangeSlider(name=\"n-grams\", value=(1, 1), start=1, end=5, step=1)\n",
    "max_df = pnw.FloatSlider(name=\"max_df\", value=1.0, start=0, end=1, step=0.01)\n",
    "min_df = pnw.FloatSlider(name=\"min_df\", value=1, start=0, end=1, step=0.01)\n",
    "max_features = pnw.IntSlider(name=\"max features\", value=500, start=50, end=10000, step=50)\n",
    "dis_metric = pnw.Select(name=\"Distance Metric\", value=\"euclidean\", options=metrices)\n",
    "\n",
    "@pn.depends(folder_name.param.value, metadata_name.param.value, analyzer.param.value, n_gram.param.value, max_df.param.value, min_df.param.value, max_features.param.value, dis_metric.param.value)\n",
    "def get_matrix(folder_name, metadata_name, analyzer, n_gram, max_df, min_df, max_features, dis_metric):\n",
    "    texts_path = \"texts/\" + folder_name\n",
    "    files_path = [texts_path + \"/\" + x for x in os.listdir(texts_path)]\n",
    "    file_keys = [os.path.basename(x).split(\".\")[0] for x in files_path]   \n",
    "    metadata_path = \"metadata/\" + metadata_name\n",
    "    metadata = pd.read_csv(metadata_path).set_index(\"text_name\")\n",
    "    \n",
    "    vectorization = vectorize(files_path, analyzer, n_gram, max_df, min_df, max_features, file_keys)\n",
    "    distance_matrix = distance_calculator(vectorization[0], dis_metric, file_keys)\n",
    "    \n",
    "    len_vocab = pnw.StaticText(name=\"Number of Vocabulary Words\", value=len(vectorization[2].index))\n",
    "    row1 = pn.Column(len_vocab, vectorization[2].sort_values(\"feature index\"))#.servable(\"Cross-selector\")\n",
    "    row2 = pn.Row(px.imshow(distance_matrix))\n",
    "    tabs = pn.Tabs((\"Vocabulary\", row1), (\"Distance matrix\", row2), dynamic=True)\n",
    "    return tabs\n",
    "\n",
    "widgets = pn.WidgetBox(folder_name, metadata_name, analyzer, n_gram, max_df, min_df, max_features, dis_metric)\n",
    "col = pn.Column(widgets)\n",
    "tabs = pn.Row(col, get_matrix)\n",
    "tabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Dimension Reduction\n",
    "\n",
    "In this part of the notebook, we are going to use t-SNE to reduce our multi-dimensional matrix into a two-dimensions, which we can put on a scatter plot to get a simplified view of groupings of similar texts in the corpora.\n",
    "\n",
    "t-SNE is a probablistic method that tries to find the best way to reduce dimensions with as little distortion and information loss as possible. Because it is based on probabilities, everytime the code is run, The resulting scatter plot will look slightly different, even when the same parameters are used. While the exact placements of the texts in the scatter plot will change, the relative distance between texts usually stays more or less the same.\n",
    "\n",
    "The code below uses the default settings for t-SNE in sklearn, except raising the number of iterations from 1,000 to 5,000 (), and the following two parameters which are left to the user:\n",
    "\n",
    "There are two main parameters to choose:\n",
    "\n",
    "1. perplexity\n",
    "2. n_iter\n",
    "3. distance metric\n",
    "\n",
    "In addition to the explanations below, the following resource [following resource](https://distill.pub/2016/misread-tsne/) can be beneficial to get a better intuition on how the parameters affect the results.\n",
    "\n",
    "### perplexity\n",
    "\n",
    "The recommended size of perplexity changes depending on the size of the corpus under study. Generally speaking, the larger the corpus (the more dimensions there are), the higher perplexity is needed to get good results that represent the corpus well in 2D space. It is defined as follows in sklearn's documentation:\n",
    "\n",
    "> The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results.\n",
    "\n",
    "However, some have shown that for truly large databases, even larger perplexity than 50 is recommended.\n",
    "\n",
    "A useful rule of thumb to begin with for choosing perplexity, is the to use the number of dimensions by the power of 0.5. In our case, the number of dimensions is the number of texts in the corpus. If there are, for example, 500 texts, 500 by the power of 0.5 is ca. 22.36, so a good starting perplexity would be about 22 or 23. In the case of the texts published in Dillard's dissertation, there are 81 documents, so a good perplexity to start with is 9.\n",
    "\n",
    "The following code block automatically calculates this recommended rule of thumb, but you can also change it if you wish.\n",
    "\n",
    "### n_iter\n",
    "\n",
    "This parameter is the number of iterations the t-SNE will run. Each iteration attempts to place the different datapoint on a 2D plot in the best way possible. The default setting in t-SNE in sklearn in 1000. Generally speaking, a large number of iterations is recommended: when there are two few, the data will not divide to meaningful groupings. The main disadvantage of large datasets is that a large number of iterations will take significantly longer to compute.\n",
    "\n",
    "For the texts published by Dillard, 5000 iterations were found to be sufficient for reproducible results.\n",
    "\n",
    "### distance metric\n",
    "\n",
    "The same metrices that were used before are available here:\n",
    "\n",
    "‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘jensenshannon’, ‘kulsinski’, ‘kulczynski1’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n",
    "\n",
    "The First code block below calculates the distance matrix based on the latest settings from the previous code block (if you change the settings there, you will need to rerun this codeblock to apply the changes to the t-SNE visualization). Then a widget will appear in which you can choose the parameters for the t-SNE.\n",
    "\n",
    "Then running the code block below it will display the results of the t-SNE in a 2D scatter plot. You can change the color and the shape of the points in the scatter plot according to information from the metadata, to see if the calculations managed to detect meaningful groupings.\n",
    "\n",
    "Red warning messages may appear after running the second cell: if the scatter plot eventually displays, they can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting needed values from previous\n",
    "metrices = ['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'jensenshannon', 'kulsinski', 'kulczynski1', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
    "metadata_path = \"metadata/\" + metadata_name.value\n",
    "metadata = pd.read_csv(metadata_path).set_index(\"text_name\")\n",
    "texts_path = \"texts/\" + folder_name.value\n",
    "files_path = [texts_path + \"/\" + x for x in os.listdir(texts_path)]\n",
    "file_keys = [os.path.basename(x).split(\".\")[0] for x in files_path]\n",
    "vectorization = vectorize(files_path, analyzer.value, n_gram.value, max_df.value, min_df.value, max_features.value, file_keys)\n",
    "distance_matrix = distance_calculator(vectorization[0], dis_metric.value, file_keys)\n",
    "\n",
    "#creating tsne widget box and button\n",
    "tsne_metric_widg = pnw.Select(name=\"T-SNE Metric\", value=\"euclidean\", options=metrices)\n",
    "n_iter_widg = pnw.IntSlider(name=\"Number of Iterations\", value=5000, start=500, end=10000, step=100)\n",
    "perplexity_widg = pnw.IntSlider(name=\"Perplexity\", value=int(round(len(file_keys)**0.5,0)), start=5, end=50, step=1)\n",
    "\n",
    "pn.WidgetBox(tsne_metric_widg, n_iter_widg, perplexity_widg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set t-sne parameters for existing values in boxes above\n",
    "tsne_metric = tsne_metric_widg.value\n",
    "n_iter = n_iter_widg.value\n",
    "perplexity = perplexity_widg.value\n",
    "\n",
    "distance_tsne = reduce_dimensions(distance_matrix, file_keys, perplexity, n_iter, tsne_metric)\n",
    "distance_2Dplot = pd.concat([distance_tsne[0], metadata], axis=1)\n",
    "\n",
    "marker_labels = [\"circle\", \"triangle\", \"diamond\", \"square\", \"plus\", \"star\", \"hex\", \"inverted_triangle\", \"circle_cross\", \"circle_dot\", \"circle_x\", \"circle_y\", \"triangle_dot\", \"triangle_pin\", \"diamond_dot\", \"diamond_cross\", \"square_cross\", \"square_dot\", \"square_pin\", \"square_x\", \"star_dot\", \"hex_dot\"]\n",
    "marker_columns = [col for col in metadata if len(metadata[col].unique()) < len(marker_labels)]\n",
    "\n",
    "color = pnw.Select(name=\"Color\", value=list(metadata.columns)[0], options=list(metadata.columns))\n",
    "marker = pnw.Select(name=\"Shape\", value=marker_columns[0], options=marker_columns)\n",
    "\n",
    "@pn.depends(color.param.value, marker.param.value)\n",
    "def create_tsne(color, marker):\n",
    "    opts = dict(cmap=\"Category20\", width=700, height=600, line_color=\"black\", size=10, tools=[\"hover\"], legend_position=\"left\", color=color, marker=hv.dim(marker).categorize(marker_labels))\n",
    "    return hv.Points(distance_2Dplot, [\"component 1\", \"component 2\"]).opts(**opts)\n",
    "\n",
    "def save(event):\n",
    "    t = get_time()\n",
    "    folder = \"experiments/\" + max([f for f in os.listdir(\"experiments/\")], key=lambda x: os.stat(os.path.join(\"experiments/\",x)).st_ctime)\n",
    "    tsne_title = \"/distance tsne perplexity\"+str(perplexity)+\" metric-\"+tsne_metric+\" n-iter\"+str(n_iter)+\" kl-divergence\"+str(round(distance_tsne[1], 2))+\" \"+t\n",
    "    distance_2Dplot.to_csv(folder+tsne_title+\".csv\", encoding=\"utf-8\")\n",
    "    graph = hv.Points(distance_2Dplot, [\"component 1\", \"component 2\"]).opts(cmap=\"Category20\", width=700, height=600, line_color=\"black\", size=10, tools=[\"hover\"], legend_position=\"left\", color=color.value, marker=hv.dim(marker.value).categorize(marker_labels))\n",
    "    hv.save(graph, folder+tsne_title+\".html\", fmt=\"html\")\n",
    "    #hv.save(graph, folder+tsne_title+\".png\", fmt=\"png\")\n",
    "    \n",
    "    with open(folder+\"/\"+\"T-SNE documentation\"+t+\".md\", \"a\", encoding=\"utf-8\") as doc:\n",
    "        doc.write(f\"## T-SNE Experiment run at {t}\")\n",
    "        doc.write(f\"\\nmetric: {tsne_metric}\")\n",
    "        doc.write(f\"\\nperplexity: {perplexity}\")\n",
    "        doc.write(f\"\\nn_iter: {n_iter}\")\n",
    "        doc.write(f\"\\nKullback-Leibler divergence after optimization: {distance_tsne[1]}\")\n",
    "        doc.write(f\"\\nnumber of features seen during fit: {distance_tsne[2]}\\n\\n\")\n",
    "    \n",
    "    col.append(pnw.StaticText(value=f\"Your T-SNE results were saved successfully! You will find a new CSV under the same experiments folder ({folder}), and the parameters for this t-SNE were documented in a documentation markdown file.\"))\n",
    "\n",
    "button = pnw.Button(name=\"Save current results and settings\", button_type=\"primary\")\n",
    "button.on_click(save)\n",
    "widgets = pn.WidgetBox(color, marker)\n",
    "col = pn.Column(widgets, button)\n",
    "pn.Row(col, create_tsne).servable(\"Cross-selector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Network Analysis\n",
    "\n",
    "Another way of looking at relationships between the texts in the corpus, is to map their relations on a graph.\n",
    "\n",
    "The code belows creates edge links between the texts, or nodes, in the corpus. Edges are created between each node and a subset of the texts that are closest to it. The number of closest texts is chosen by running the first code block below.\n",
    "\n",
    "Running the second code block will visualize the network. The nodes can be colored based on the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nearest_texts = pnw.IntSlider(name=\"Number of Nearest Texts to consider\", value=3, start=1, end=10, step=1)\n",
    "\n",
    "pn.WidgetBox(n_nearest_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dictionary that holds the edges\n",
    "\n",
    "texts = distance_matrix.index.tolist()\n",
    "num = n_nearest_texts.value\n",
    "edges_dict = []\n",
    "\n",
    "for text in texts:\n",
    "    target_texts_df = distance_matrix.nsmallest(num, text)[1:]\n",
    "    target_texts = target_texts_df.index.tolist()\n",
    "    \n",
    "    for target in target_texts:\n",
    "        weight = target_texts_df.loc[target][text]\n",
    "        edges_dict.append({\"source\": text, \"target\": target, \"weight\": weight})\n",
    "\n",
    "## normalizing weights so that higher values would reflect closer connections between texts\n",
    "weights = [edge[\"weight\"]**2 for edge in edges_dict]\n",
    "max_weight = max(weights)+1\n",
    "for edge in edges_dict:\n",
    "    edge[\"weight\"] = max_weight - edge[\"weight\"]\n",
    "        \n",
    "edges_df = pd.DataFrame.from_records(edges_dict)\n",
    "\n",
    "# create a network with networkx\n",
    "G = nx.from_pandas_edgelist(edges_df, \"source\", \"target\", \"weight\", create_using=nx.DiGraph)\n",
    "\n",
    "# create metadata for the network\n",
    "\n",
    "def get_metadata_for_node(metadata_df, metadata_type, nodes):\n",
    "    metadata_dict = {}\n",
    "    for node in nodes:\n",
    "        metadata_dict[node] = metadata_df.loc[node][metadata_type] #\"FLP \"+str(node)\n",
    "    return metadata_dict\n",
    "\n",
    "# set node attributes\n",
    "\n",
    "for meta in metadata.columns:\n",
    "    data = get_metadata_for_node(metadata, meta, G.nodes)\n",
    "    nx.set_node_attributes(G, name=meta, values=data)\n",
    "\n",
    "nx.set_node_attributes(G, name=\"Degree\", values=dict(nx.degree(G)))\n",
    "\n",
    "positions = nx.layout.fruchterman_reingold_layout(G)\n",
    "\n",
    "color1 = pnw.Select(name=\"Node color 1\", value=list(metadata.columns)[0], options=list(metadata.columns))\n",
    "color2 = pnw.Select(name=\"Node color 2\", value=list(metadata.columns)[1], options=list(metadata.columns))\n",
    "\n",
    "def opts_settings(node_color, title):\n",
    "    return opts.Graph(directed=True, node_size=\"Degree\", arrowhead_length=0.005, width=400, height=400, node_color=node_color, cmap=\"Category20\", edge_color=\"grey\", edge_line_width=\"weight\", show_legend=True, title=title)\n",
    "\n",
    "@pn.depends(color1.param.value, color2.param.value)\n",
    "def make_networks(color1, color2):\n",
    "    network1 = hv.Graph.from_networkx(G, positions)\n",
    "    network2 = hv.Graph.from_networkx(G, positions)\n",
    "    network1.opts(opts_settings(color1, color1))\n",
    "    network2.opts(opts_settings(color2, color2))\n",
    "    return hv.Layout(network1 + network2)\n",
    "\n",
    "widgets = pn.WidgetBox(color1, color2)\n",
    "pn.Column(widgets, make_networks).servable(\"Cross-selector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: close reading\n",
    "\n",
    "What makes texts in the corpus similar to each other, from a computational perspective?\n",
    "\n",
    "In the first code block below, you can choose the text name you want to compare to, and the number of most similar texts to it. That number will include the text chosen for comparison - meaning, if 5 is chosen, the five texts will be the one chosen and its 4 nearest texts. You can then see a table with infromation from the metadata that may or may not explain the similarities.\n",
    "\n",
    "The code block below it, looks at shared terms used in pairs of documents. It creates a scatter plot in which the points are terms which appear in both values. The x-axis is the tf-idf values of the term in the first text, and the y-axis is the tf-idf values of the same term in the second text. In some pairs of texts that are particularly similar, it is possible to see that their shared terms create a vector on the scatter plot, indicating key terms share similar tf-idf in similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = pnw.Select(name=\"Text\", value=file_keys[0], options=file_keys)\n",
    "num = pnw.IntInput(name=\"Number of Comparisons\", value=5, start=1, end=len(file_keys))\n",
    "\n",
    "@pn.depends(text.param.value, num.param.value)\n",
    "def compare_ntexts(text, num):\n",
    "    nearest_texts = distance_matrix.nsmallest(num, text)[text]\n",
    "    nearest_texts_metadata = pd.concat([nearest_texts, metadata], axis=1, join=\"inner\")\n",
    "    #pd.options.display.max_columns = None\n",
    "    #display(nearest_texts_metadata)\n",
    "    tabulator_editors = {col:None for col in nearest_texts_metadata.columns}\n",
    "    return pn.widgets.Tabulator(nearest_texts_metadata, widths=100, editors=tabulator_editors)\n",
    "\n",
    "\n",
    "widgets = pn.WidgetBox(text, num, width=200)\n",
    "pn.Row(widgets, compare_ntexts).servable(\"Cross-selector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compare Two Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = pnw.Select(name=\"Text 1\", value=file_keys[0], options=file_keys)\n",
    "text2 = pnw.Select(name=\"Text 2\", value=file_keys[1], options=file_keys)\n",
    "\n",
    "@pn.depends(text1.param.value, text2.param.value)\n",
    "def compare2texts(text1, text2):\n",
    "    opts = dict(width=500, height=500, size=10, tools=[\"hover\"])\n",
    "    if text1 != None and text2 != None:\n",
    "        comparison_df = vectorization[1].loc[[text1, text2]].transpose()\n",
    "        comparison_df = comparison_df[comparison_df[text1] != 0]\n",
    "        comparison_df = comparison_df[comparison_df[text2] != 0]\n",
    "        comparison_df[\"token\"] = comparison_df.index \n",
    "\n",
    "        return hv.Points(comparison_df).opts(**opts)\n",
    "    \n",
    "widgets = pn.WidgetBox(text1, text2, width=200, height=600)\n",
    "pn.Row(widgets, compare2texts).servable(\"Cross-selector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
